# -*- coding: utf-8 -*-
"""text_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_t24Ymmrg2nuSqSLfBQibbT6Anp0RENi
"""

#Libraries
import os
import nltk
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

nltk.download('words')
from nltk.corpus import words


from textblob import TextBlob

#Tokenizer


#Read the file from drive

student_course_feedback = open('/content/drive/MyDrive/Information Retrieval/student_course_feedback.txt')

twitter= open('/content/drive/MyDrive/Information Retrieval/twitter_data.txt')

research_paper= open('/content/drive/MyDrive/Information Retrieval/research_paper.txt')

#student_course_feedback
student_course_feedback = student_course_feedback.read()  # read entire file
student_course_feedback_tokenized=word_tokenize(student_course_feedback)
print(student_course_feedback_tokenized)
print("--------------------------")

#Twitter
twitter = twitter.read() 
twitter_tokenized=word_tokenize(twitter)
print(twitter_tokenized)
print("--------------------------")

#research paper

research_paper = research_paper.read() 
research_paper_tokenized=word_tokenize(research_paper)
print(research_paper_tokenized)
print("--------------------------")

#Spell Correction


# Calling TextBlob function
print("-----------Student course feedback---------------")
student_course_feedback_blob = TextBlob(student_course_feedback) 
corrected_data = student_course_feedback_blob.correct()
print(corrected_data)

print("-----------Twitter---------------")
twitter_blob = TextBlob(twitter) 
corrected_data = twitter_blob.correct()
print(corrected_data)


print("-----------Research Paper---------------")
research_paper_blob = TextBlob(research_paper) 
corrected_data = research_paper_blob.correct()
print(corrected_data)

#Stemming


#Get the tokenized text and iterate through each word

stemmer = PorterStemmer()

# Printing the stem word
print("-----------Student course feedback---------------")
for word in student_course_feedback_tokenized:
	rootWord=stemmer.stem(word)
	print("Stem word for {} is {}".format(word,rootWord))
 

print("-----------Twitter---------------")
for word in twitter_tokenized:
	rootWord=stemmer.stem(word)
	print("Stem word for {} is {}".format(word,rootWord))

print("-----------Research paper---------------")
for word in research_paper_tokenized:
	rootWord=stemmer.stem(word)
	print("Stem word for {} is {}".format(word,rootWord))

#Lemmatization


# Call the lemmatizer
lemmatizer = WordNetLemmatizer()
print("--------------Student_course_feedback----------------------")
for word in student_course_feedback_tokenized:
    print("Lemma for {} is {}".format(word, lemmatizer.lemmatize(word))) 

print("--------------Twitter----------------------")
for word in  twitter_tokenized:
    print("Lemma for {} is {}".format(word, lemmatizer.lemmatize(word))) 
    
print("--------------Rsearch paper----------------------")
for word in  research_paper_tokenized:
    print("Lemma for {} is {}".format(word, lemmatizer.lemmatize(word)))